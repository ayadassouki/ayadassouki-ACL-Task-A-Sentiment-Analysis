{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayadassouki/ayadassouki-ACL-Task-A-Sentiment-Analysis/blob/main/gemini_fineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzXCGnH8pH60"
      },
      "outputs": [],
      "source": [
        "#installing dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLOe9BEBp0Ih"
      },
      "outputs": [],
      "source": [
        "# api key set up\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('new_key_semeval')\n",
        "genai.configure(api_key= api_key)\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "prompt = 'this is a test prompt, generate anything'\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS1NnAnusRFK"
      },
      "source": [
        "**Reading the data and making functions to turn to json and llm specific formats**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbikVrvTp8cj"
      },
      "outputs": [],
      "source": [
        "dataframe = pd.read_csv('eng.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwZlHn25qNfn"
      },
      "outputs": [],
      "source": [
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PinFBcc9qQro"
      },
      "outputs": [],
      "source": [
        "dataframe.drop(['id'], inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSsGMYRZqWLM"
      },
      "outputs": [],
      "source": [
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnLKVJIYqYLU"
      },
      "outputs": [],
      "source": [
        "# split the data into training, validation and testing data\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, temp_df = train_test_split(dataframe, test_size=0.4, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUrO4EbxqkNn"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMYqCH1nqlQl"
      },
      "outputs": [],
      "source": [
        "len(train_df), len(test_df), len(val_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXX5Ulp4qw6u"
      },
      "outputs": [],
      "source": [
        "# convert data frame to json\n",
        "def df_to_json(df):\n",
        "  json_data = df.to_json(orient='records')\n",
        "  dict_data = json.loads(json_data)\n",
        "  return dict_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nkc-LkEr3j4"
      },
      "outputs": [],
      "source": [
        "def input_output(json_data):\n",
        "    formatted_data = []\n",
        "    for entry in json_data:\n",
        "        # Extract the text input\n",
        "        text = entry['text']\n",
        "        # Flatten the emotion dictionary into a single string\n",
        "        emotions = \", \".join([f\"{key}: {value}\" for key, value in entry.items() if key in [\"Anger\", \"Fear\", \"Joy\", \"Sadness\", \"Surprise\"]])\n",
        "        # Append the formatted input-output pair\n",
        "        formatted_data.append({\"text_input\": text, \"output\": emotions})\n",
        "    return formatted_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-3HOqcVr66G"
      },
      "outputs": [],
      "source": [
        "train_json = df_to_json(train_df)\n",
        "train_inout = input_output(train_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EK0evP7psEaa"
      },
      "outputs": [],
      "source": [
        "len(train_inout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kN6tJ4DsHfk"
      },
      "outputs": [],
      "source": [
        "train_inout[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUYl8v58secK"
      },
      "source": [
        "**Fine Tuning Gemini**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApS2EdzxsJic"
      },
      "outputs": [],
      "source": [
        "# fine tuning gemini\n",
        "import time\n",
        "\n",
        "# Define base model and prepare training data\n",
        "base_model = \"models/gemini-1.5-flash-001-tuning\"\n",
        "\n",
        "# Start fine-tuning\n",
        "operation = genai.create_tuned_model(\n",
        "    display_name=\"sentiment_analysis_finetune\",\n",
        "    source_model=base_model,\n",
        "    epoch_count=10,  # Adjust epochs as needed\n",
        "    batch_size=4,    # Adjust batch size as needed\n",
        "    learning_rate=0.001,\n",
        "    training_data=train_inout\n",
        ")\n",
        "\n",
        "# Wait for the fine-tuning process to complete\n",
        "for status in operation.wait_bar():\n",
        "    time.sleep(10)\n",
        "\n",
        "# Retrieve the tuned model details\n",
        "result = operation.result()\n",
        "print(f\"Fine-tuned model created: {result.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxGyt8bdcnXj"
      },
      "outputs": [],
      "source": [
        "def input_val_test(json_data):\n",
        "  formatted_data = []\n",
        "  for entry in json_data:\n",
        "    text = entry['text']\n",
        "    emotions = \"Anger: 0, Fear: 0, Joy: 0, Sadness: 0, Surprise: 0\"\n",
        "    formatted_data.append({\"text_input\": text})\n",
        "  return formatted_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ3DDLg0KSsv"
      },
      "outputs": [],
      "source": [
        "val_json = df_to_json(val_df)\n",
        "val_inout = input_val_test(val_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "479uxOcKLEDw"
      },
      "outputs": [],
      "source": [
        "val_inout[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPutPJGdskNa"
      },
      "source": [
        "**Getting Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2EyvR26LE3w"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Initialize predictions list\n",
        "predictions = []\n",
        "\n",
        "# Define the fine-tuned model name\n",
        "fine_tuned_model = \"tunedModels/sentimentanalysisfinetune-bxndg9w0jfgu\"\n",
        "model = genai.GenerativeModel(fine_tuned_model)\n",
        "\n",
        "# Total number of entries\n",
        "total_entries = len(val_inout)\n",
        "# Iterate through validation data\n",
        "for i, entry in enumerate(val_inout):\n",
        "# Construct prompt\n",
        "    prompt = f\"\"\"\n",
        "    You are a fine-tuned model trained to predict emotions in a given text. Always provide the output strictly in the specified dictionary format.\n",
        "\n",
        "    Analyze the following text:\n",
        "    {entry}\n",
        "\n",
        "    Your response must be exactly in this format:\n",
        "    {{\n",
        "      'text_input': 'text you did analysis on',\n",
        "      'output': 'Anger: , Fear: , Joy: , Sadness: , Surprise: '\n",
        "    }}\n",
        "\n",
        "\n",
        "    Only return the output in the specified format. Do not add or remove anything.\n",
        "    All emotion tags must be present in the output.\n",
        "\n",
        "    Now process the input text and provide your response.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Generate content using the fine-tuned model\n",
        "        response = model.generate_content(prompt)\n",
        "        predictions.append(response.text)  # Store response\n",
        "    except Exception as e:\n",
        "        print(f\"Error for entry {i + 1}/{total_entries}: {e}\")  # Log error details\n",
        "\n",
        "    # Display progress\n",
        "    progress = ((i + 1) / total_entries) * 100\n",
        "    print(f\"Progress: {i + 1}/{total_entries} ({progress:.2f}%)\")\n",
        "\n",
        "    # Adjust delay based on API rate limits\n",
        "    time.sleep(10)  # Set this to match the API's recommended rate limit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aM862_xHLqmh"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjac-vzdLvRh"
      },
      "outputs": [],
      "source": [
        "len(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU5WOWWpss3o"
      },
      "source": [
        "**Cleaning LLM Predictions into dataframe acceptable format**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF1Woz_BPtUf"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_llm_responses(responses):\n",
        "    cleaned_responses = []\n",
        "\n",
        "    for response in responses:\n",
        "        # Step 1: Remove unwanted prefixes (e.g., \"0: 0\" or \"1.\")\n",
        "        response = re.sub(r\"^\\d+[:.]\\s*\", \"\", response)\n",
        "\n",
        "        # Step 2: Remove duplicate emotion keys (keep the first occurrence)\n",
        "        keys_seen = set()\n",
        "        clean_response = []\n",
        "        for pair in response.split(\",\"):\n",
        "            key_value = pair.strip()\n",
        "            key = key_value.split(\":\")[0].strip()\n",
        "            if key not in keys_seen:\n",
        "                keys_seen.add(key)\n",
        "                clean_response.append(key_value)\n",
        "\n",
        "        # Step 3: Extract key-value pairs using a regex\n",
        "        matches = re.findall(r\"(Anger|Fear|Joy|Sadness|Surprise):\\s*(\\d)\", \", \".join(clean_response))\n",
        "\n",
        "        # Step 4: Create a standardized dictionary\n",
        "        emotions = {key: int(value) for key, value in matches}\n",
        "\n",
        "        # Step 5: Ensure all keys are present with default value 0\n",
        "        all_emotions = {\"Anger\": 0, \"Fear\": 0, \"Joy\": 0, \"Sadness\": 0, \"Surprise\": 0}\n",
        "        all_emotions.update(emotions)\n",
        "\n",
        "        # Append the cleaned response\n",
        "        cleaned_responses.append(all_emotions)\n",
        "\n",
        "    return cleaned_responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_t6n5_MoW4k"
      },
      "outputs": [],
      "source": [
        "cleaned_response = preprocess_llm_responses(predictions)\n",
        "cleaned_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dlkxq2t-oxH7"
      },
      "outputs": [],
      "source": [
        "validation_pred_df = pd.DataFrame(cleaned_response)\n",
        "validation_pred_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpmzHpySs1E2"
      },
      "source": [
        "**Adjusting the data for some missing values (18 in this case)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQoQQ13Dqnve"
      },
      "outputs": [],
      "source": [
        "def drop_missing_indices_direct(data, missing_indices):\n",
        "    \"\"\"\n",
        "    Removes entries from a list based on indices, adjusting for 1-based to 0-based indexing.\n",
        "\n",
        "    Parameters:\n",
        "    - data (list): Original list of dictionaries or items.\n",
        "    - missing_indices (list): 1-based indices of entries to remove.\n",
        "\n",
        "    Returns:\n",
        "    - List with the specified indices removed.\n",
        "    \"\"\"\n",
        "    # Convert 1-based indices to 0-based\n",
        "    zero_based_indices = [idx - 1 for idx in missing_indices]\n",
        "    return [data[i] for i in range(len(data)) if i not in zero_based_indices]\n",
        "\n",
        "missing_indices = [481, 527, 533, 536, 537, 538, 542, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554]\n",
        "\n",
        "# Drop missing entries\n",
        "filtered_val_inout = drop_missing_indices_direct(val_inout, missing_indices)\n",
        "\n",
        "# Display results\n",
        "print(f\"Total entries before: {len(val_inout)}\")\n",
        "print(f\"Total entries after dropping: {len(filtered_val_inout)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f0pYv90r2rI"
      },
      "outputs": [],
      "source": [
        "filtered_val_inout[480]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyS-62XatfYS"
      },
      "outputs": [],
      "source": [
        "v_df = validation_pred_df.assign(text_input=filtered_val_inout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfxeZBDhuEqE"
      },
      "outputs": [],
      "source": [
        "v_df['text_input'] = v_df['text_input'].apply(lambda x: x['text_input'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iHM5v8RuFRE"
      },
      "outputs": [],
      "source": [
        "v_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynIodYiWujFY"
      },
      "outputs": [],
      "source": [
        "v_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSCKyK2UvBkE"
      },
      "outputs": [],
      "source": [
        "v_df.rename(columns={'Anger': 'Pred_Anger', 'Fear': 'Pred_Fear', 'Joy':'Pred_Joy', 'Sadness':'Pred_Sadness', 'Surprise':'Pred_Surprise'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3ubKaCtvkqf"
      },
      "outputs": [],
      "source": [
        "v_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1hfLPSEvl8D"
      },
      "outputs": [],
      "source": [
        "# Instead of using missing_indices directly, get the actual index values to drop\n",
        "missing_org_indices = [480, 526, 532, 535, 536, 537, 541, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553]\n",
        "indices_to_drop = val_df.index[missing_org_indices]\n",
        "\n",
        "# Now drop the rows using the correct index values\n",
        "val_df.drop(indices_to_drop, inplace=True, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHDvfDaGxIZS"
      },
      "outputs": [],
      "source": [
        "val_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rebLq8jVz8j1"
      },
      "outputs": [],
      "source": [
        "v_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTh0-gB40wdY"
      },
      "outputs": [],
      "source": [
        "val_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9k_B9Nw1fZE"
      },
      "outputs": [],
      "source": [
        "# Columns to add\n",
        "columns_to_add = ['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
        "\n",
        "# Iterate over columns_to_add and set the corresponding values from val_df\n",
        "for col in columns_to_add:\n",
        "    v_df[col] = val_df[col].values\n",
        "\n",
        "# Display the updated v_df\n",
        "print(v_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ0U4UQU1ijg"
      },
      "outputs": [],
      "source": [
        "v_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WBJ2Iyd2End"
      },
      "outputs": [],
      "source": [
        "for col in columns_to_add:\n",
        "  v_df[col].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD7mL3Hu2jUj"
      },
      "outputs": [],
      "source": [
        "v_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W9Tz5GDtAhm"
      },
      "source": [
        "**Classification report for Validation Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WOgP02v2kUD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming you have true labels in the DataFrame as 'true_anger', 'true_fear', etc.\n",
        "true_labels = ['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
        "pred_emotion = ['Pred_Anger', 'Pred_Fear', 'Pred_Joy', 'Pred_Sadness', 'Pred_Surprise']\n",
        "\n",
        "# Create an empty dictionary to store classification reports for each emotion\n",
        "classification_reports = {}\n",
        "\n",
        "# Iterate through each emotion and calculate the classification report\n",
        "for true_label, pred_label in zip(true_labels, pred_emotion):\n",
        "    true_values = v_df[true_label]  # True labels for the emotion\n",
        "    pred_values = v_df[pred_label]  # Predicted labels for the emotion\n",
        "\n",
        "\n",
        "    # Generate the classification report\n",
        "    report = classification_report(true_values, pred_values)\n",
        "\n",
        "    # Store the report in the dictionary\n",
        "    classification_reports[true_label] = report\n",
        "\n",
        "# Print the classification reports\n",
        "for emotion, report in classification_reports.items():\n",
        "    print(f\"Classification Report for {emotion}:\")\n",
        "    print(report)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l43wGkM3tHPD"
      },
      "source": [
        "**Making predictions for Testing Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAD6aegA3And"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF8K1BDtR6YG"
      },
      "outputs": [],
      "source": [
        "test_inout = input_val_test(df_to_json(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "488GdGqXR9E0"
      },
      "outputs": [],
      "source": [
        "test_inout[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GXnOt-hR-SQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Initialize predictions list\n",
        "test_predictions = []\n",
        "\n",
        "# Define the fine-tuned model name\n",
        "fine_tuned_model = \"tunedModels/sentimentanalysisfinetune-bxndg9w0jfgu\"\n",
        "model = genai.GenerativeModel(fine_tuned_model)\n",
        "\n",
        "# Total number of entries\n",
        "total_entries = len(test_inout)\n",
        "# Iterate through validation data\n",
        "for i, entry in enumerate(test_inout):\n",
        "# Construct prompt\n",
        "    test_prompt = f\"\"\"\n",
        "    You are a fine-tuned model trained to predict emotions in a given text. Always provide the output strictly in the specified dictionary format.\n",
        "\n",
        "    Analyze the following text:\n",
        "    {entry}\n",
        "\n",
        "    Your response must be exactly in this format:\n",
        "    {{\n",
        "      'text_input': 'text you did analysis on',\n",
        "      'output': 'Anger: , Fear: , Joy: , Sadness: , Surprise: '\n",
        "    }}\n",
        "\n",
        "\n",
        "    Only return the output in the specified format. Do not add or remove anything.\n",
        "    All emotion tags must be present in the output.\n",
        "\n",
        "    Now process the input text and provide your response.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Generate content using the fine-tuned model\n",
        "        test_response = model.generate_content(test_prompt)\n",
        "        test_predictions.append(test_response.text)  # Store response\n",
        "    except Exception as e:\n",
        "        print(f\"Error for entry {i + 1}/{total_entries}: {e}\")  # Log error details\n",
        "\n",
        "    # Display progress\n",
        "    progress = ((i + 1) / total_entries) * 100\n",
        "    print(f\"Progress: {i + 1}/{total_entries} ({progress:.2f}%)\")\n",
        "\n",
        "    # Adjust delay based on API rate limits\n",
        "    time.sleep(10)  # Set this to match the API's recommended rate limit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTHuNpOaSaFz"
      },
      "outputs": [],
      "source": [
        " test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAMbO9ycpQWj"
      },
      "outputs": [],
      "source": [
        "len(test_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dvShEFmpV20"
      },
      "outputs": [],
      "source": [
        "clean_testing = preprocess_llm_responses(test_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPBcKg8Mpqba"
      },
      "outputs": [],
      "source": [
        "clean_testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0oh1lZ7p0O7"
      },
      "outputs": [],
      "source": [
        "len(clean_testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo5Owu0Yp5Z6"
      },
      "outputs": [],
      "source": [
        "testing_df = pd.DataFrame(clean_testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSOBveKbp8eG"
      },
      "outputs": [],
      "source": [
        "testing_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hNc8NlSqUj-"
      },
      "outputs": [],
      "source": [
        "t_df = testing_df.assign(text_input=test_inout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tilabg0Pp-jH"
      },
      "outputs": [],
      "source": [
        "t_df['text_input'] = t_df['text_input'].apply(lambda x: x['text_input'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3NS4vytqR4n"
      },
      "outputs": [],
      "source": [
        "t_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6TR7HQFqlhw"
      },
      "outputs": [],
      "source": [
        "t_df.rename(columns={'Anger': 'Pred_Anger', 'Fear': 'Pred_Fear', 'Joy':'Pred_Joy', 'Sadness':'Pred_Sadness', 'Surprise':'Pred_Surprise'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRc33XKRqoFh"
      },
      "outputs": [],
      "source": [
        "t_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOdrIK5Sqptg"
      },
      "outputs": [],
      "source": [
        "# Columns to add\n",
        "columns_to_add = ['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
        "\n",
        "# Iterate over columns_to_add and set the corresponding values from val_df\n",
        "for col in columns_to_add:\n",
        "    t_df[col] = test_df[col].values\n",
        "\n",
        "# Display the updated v_df\n",
        "print(t_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXqKScRBqwqI"
      },
      "outputs": [],
      "source": [
        "t_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPrWREnhqygr"
      },
      "outputs": [],
      "source": [
        "for col in columns_to_add:\n",
        "  t_df[col].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL6hhlf3q5WB"
      },
      "outputs": [],
      "source": [
        "t_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBcBFlQLtRev"
      },
      "source": [
        "**Classification Report for Testing Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlsxV8yaq6f0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Assuming you have true labels in the DataFrame as 'true_anger', 'true_fear', etc.\n",
        "true_labels = ['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
        "pred_emotion = ['Pred_Anger', 'Pred_Fear', 'Pred_Joy', 'Pred_Sadness', 'Pred_Surprise']\n",
        "\n",
        "# Create an empty dictionary to store classification reports for each emotion\n",
        "classification_reports = {}\n",
        "\n",
        "# Iterate through each emotion and calculate the classification report\n",
        "for true_label, pred_label in zip(true_labels, pred_emotion):\n",
        "    true_values = t_df[true_label]  # True labels for the emotion\n",
        "    pred_values = t_df[pred_label]  # Predicted labels for the emotion\n",
        "\n",
        "\n",
        "    # Generate the classification report\n",
        "    report = classification_report(true_values, pred_values)\n",
        "\n",
        "    # Store the report in the dictionary\n",
        "    classification_reports[true_label] = report\n",
        "\n",
        "# Print the classification reports\n",
        "for emotion, report in classification_reports.items():\n",
        "    print(f\"Classification Report for {emotion}:\")\n",
        "    print(report)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B3jchxvtWmj"
      },
      "source": [
        "Finished with Fine Tuning:🙂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMH0tvdIfpw-"
      },
      "source": [
        "**TESTING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoNErw2vtvQk"
      },
      "outputs": [],
      "source": [
        "test_dataframe = pd.read_csv('eng_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz9kf1dmfxGJ"
      },
      "outputs": [],
      "source": [
        "test_dataframe.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPmfWhp0fyup"
      },
      "outputs": [],
      "source": [
        "test_dataframe.drop('id', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2rDF3nGf4Oo"
      },
      "outputs": [],
      "source": [
        "test_dataframe.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LykB8LmXf5jH"
      },
      "outputs": [],
      "source": [
        "test_dataframe.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrG7L0EUf-cQ"
      },
      "outputs": [],
      "source": [
        "testing_inout = input_val_test(df_to_json(test_dataframe))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yu6lvhklgJsx"
      },
      "outputs": [],
      "source": [
        "testing_inout[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kEzbxfcegb_A"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Initialize predictions list\n",
        "test_predictions = []\n",
        "\n",
        "# Define the fine-tuned model name\n",
        "fine_tuned_model = \"tunedModels/sentimentanalysisfinetune-nb68ky4hhjlr\"\n",
        "model = genai.GenerativeModel(fine_tuned_model)\n",
        "\n",
        "# Total number of entries\n",
        "total_entries = len(testing_inout)\n",
        "# Iterate through validation data\n",
        "for i, entry in enumerate(testing_inout):\n",
        "# Construct prompt\n",
        "    test_prompt = f\"\"\"\n",
        "    You are a fine-tuned model trained to predict emotions in a given text. Always provide the output strictly in the specified dictionary format.\n",
        "\n",
        "    Analyze the following text:\n",
        "    {entry}\n",
        "\n",
        "    Your response must be exactly in this format:\n",
        "    {{\n",
        "      'text_input': 'text you did analysis on',\n",
        "      'output': 'Anger: , Fear: , Joy: , Sadness: , Surprise: '\n",
        "    }}\n",
        "\n",
        "\n",
        "    Only return the output in the specified format. Do not add or remove anything.\n",
        "    All emotion tags must be present in the output.\n",
        "\n",
        "    Now process the input text and provide your response.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Generate content using the fine-tuned model\n",
        "        test_response = model.generate_content(test_prompt)\n",
        "        test_predictions.append(test_response.text)  # Store response\n",
        "    except Exception as e:\n",
        "        print(f\"Error for entry {i + 1}/{total_entries}: {e}\")  # Log error details\n",
        "\n",
        "    # Display progress\n",
        "    progress = ((i + 1) / total_entries) * 100\n",
        "    print(f\"Progress: {i + 1}/{total_entries} ({progress:.2f}%)\")\n",
        "\n",
        "    # Adjust delay based on API rate limits\n",
        "    time.sleep(7)  # Set this to match the API's recommended rate limit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXRSI4SnhS1B"
      },
      "outputs": [],
      "source": [
        "for model in genai.list_tuned_models():\n",
        "    print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtoXkyFU7E5c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}